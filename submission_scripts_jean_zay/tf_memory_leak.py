# -*- coding: utf-8 -*-
"""tf_memory_leak.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qxgQdaqh8XntaFeJ3s10Ghzap4HUmkv5
"""
import h5py
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D
from tensorflow.python.ops.signal.fft_ops import ifft2d
from tqdm import tqdm

from fastmri_recon.data.utils.multicoil.smap_extract import extract_smaps
from fastmri_recon.data.datasets.multicoil.fastmri_pyfunc import train_masked_kspace_dataset_from_indexable
from fastmri_recon.models.subclassed_models.pdnet import PDNet
from fastmri_recon.models.utils.fastmri_format import tf_fastmri_format

"""## Create the fake dataset"""

# from https://github.com/googlecolab/colabtools/issues/253#issuecomment-551056637
# d=[]
# while(1):
#   d.append('1')

kspace_shape = (32, 15, 640, 372)
image_shape = (32, 320, 320)

def create_data(filename):
  kspace = np.random.normal(size=kspace_shape) + 1j * np.random.normal(size=kspace_shape)
  image = np.random.normal(size=image_shape)
  kspace = kspace.astype(np.complex64)
  image = image.astype(np.float32)
  with h5py.File(filename, "w") as h5_obj:
    h5_obj.create_dataset("kspace", data=kspace)
    h5_obj.create_dataset("reconstruction_rss", data=image)
    h5_obj.attrs['acquisition'] = 'CORPD'

def read_data(filename):
  with h5py.File(filename, 'r') as h5_obj:
    data = h5_obj['/kspace'][()]
  return data

n_files = 32
for i in tqdm(range(n_files)):
  data_filename = f"mytestfile_{i}.h5"
  create_data(data_filename)

"""## Create the data pipeline"""

# def tf_read_data(filename):
#     def _read_data(filename):
#         filename_str = filename.numpy()
#         my_data = read_data(filename_str)
#         return tf.convert_to_tensor(my_data)
#     my_data = tf.py_function(
#         _read_data,
#         [filename],
#         tf.complex64,
#     )
#     my_data.set_shape((32, 15, 640, 372))
#     return my_data

# def preprocessing(my_data):
#   # here the preprocessing is going to be very simple, in practice it could be a bit more
#   smaps = extract_smaps(my_data)
#   target_data = tf.random.normal([32, 640, 372])
#   return my_data, target_data

# files_ds = tf.data.Dataset.list_files('./*.h5')
# data_ds = files_ds.map(
#     tf_read_data,
#     num_parallel_calls=tf.data.experimental.AUTOTUNE,
# )
# val_ds = data_ds.map(
#     preprocessing,
#     num_parallel_calls=tf.data.experimental.AUTOTUNE,
# ).repeat().prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

val_ds_fastmri = train_masked_kspace_dataset_from_indexable(
    './',
    AF=4,
    contrast=None,
    inner_slices=None,
    rand=False,
    scale_factor=1e6,
    parallel=False,
)

"""## Create the fake model"""

class MyModel(Model):
  def __init__(self, **kwargs):
    super(MyModel, self).__init__(**kwargs)
    self.conv = Conv2D(1, 3, padding='same', activation='linear')

  def call(self, inputs):
    kspace, mask, smaps = inputs
    image = ifft2d(kspace[..., 0])
    rss_image = tf.norm(image, axis=1)[..., None]
    rss_image = tf_fastmri_format(rss_image)
    conv_image = self.conv(rss_image)
    return conv_image

run_params = {
    'n_primal': 5,
    'n_dual': 1,
    'primal_only': True,
    'n_iter': 10,
    'multicoil': True,
    'n_filters': 32,
}


mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    model = PDNet(**run_params)
    model([
        tf.zeros([1, 15, 640, 372, 1], dtype=tf.complex64),
        tf.zeros([1, 15, 640, 372], dtype=tf.complex64),
        tf.zeros([1, 15, 640, 372], dtype=tf.complex64),
    ])

    model.compile(loss='mse')

"""## Evaluate all of it"""

model.evaluate(val_ds_fastmri.take(2))
